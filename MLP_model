#building MLP archictecture
def create_MLP_model(output_bias=None):
    if output_bias is not None:
        output_bias = tf.keras.initializers.Constant(output_bias)
    #input layer    
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    #preprocessing layer 
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    #giving input to bert model from the pre processed layer
    encoder_inputs = preprocessing_layer(text_input)
    #selecting the encoder
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    #collecting the output of the bert encoder
    outputs = encoder(encoder_inputs)
    #collecting the pooled outputs
    net = outputs['pooled_output']
    #feeding the output to the multilayer perception
    #1 DENSE LAYER
    net = tf.keras.layers.Dense(512, activation="relu")(net)
    #1 DROPUT LAYER
    net = tf.keras.layers.Dropout(0.2)(net)
    #1 OUTPUT LAYER WITH SOFTMAX
    net = tf.keras.layers.Dense(3, activation="softmax", name='classifier', bias_initializer=output_bias)(net)
    return tf.keras.Model(text_input, net)
